{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Vehicle Detection Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Apply a color transform and append binned color features, as well as histograms of color, to HOG feature vector. \n",
    "* Normalize features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use trained classifier to search for vehicles in images.\n",
    "* Run pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image1]: ./examples/car_not_car.png\n",
    "[image2]: ./examples/HOG_example.jpg\n",
    "[image3]: ./examples/sliding_windows.jpg\n",
    "[image4]: ./examples/sliding_window.jpg\n",
    "[image5]: ./examples/bboxes_and_heat.png\n",
    "[image6]: ./examples/labels_map.png\n",
    "[image7]: ./examples/output_bboxes.png\n",
    "[video1]: ./project_video.mp4\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "\n",
    "#### 1. Features Extraction\n",
    "\n",
    "The code for this step is contained in the IPython notebook  `vehicle_detection.ipynb`.  \n",
    "\n",
    "I started by reading in all the `vehicle` and `non-vehicle` images.  Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n",
    "\n",
    "![car features](./images/car-features.png)\n",
    "\n",
    "![non-car features](./images/non-car-features.png)\n",
    "\n",
    "I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n",
    "\n",
    "Here is an example using the `YCrCb` color space and HOG parameters of `orientations=8`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n",
    "\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "#### 2. HOG parameters.\n",
    "\n",
    "I tried various combinations of parameters and settled on following parameters which resulted in best vehicle detection accuracy, with low false positives.\n",
    "\n",
    "\n",
    "```\n",
    "color_space = 'LUV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 8  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = 0 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "```\n",
    "\n",
    "#### 3. I trained a linear SVM using `sklearn`'s  `svm.LinearSVM` module with hinge loss. I chose svm due to its speed. The classifier achieved test accuracy of 0.9866, with  2432 feature vectors.\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "##### 1. Siding window search approach is implemented in function `find_cars` in `vechicle_detection.ipynb`. After exploring various search strategies, I finally settled on three window sizes. Smaller window sizes applied where vehicles would appear farther and smaller, and larger window sizes appled to search for nearer vehicles. \n",
    "\n",
    "\n",
    "![windows](./images/windows-search.png)\n",
    "\n",
    "##### 2. Examples\n",
    "\n",
    "Ultimately I searched on two scales using LUV 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.  Here are some example images:\n",
    "\n",
    "*Note: The red color boxes are the final boxes and the yellow color boxes are all detected boxes.*\n",
    "\n",
    "![test1](./output/test1.jpg)\n",
    "![test2](./output/test2.jpg)\n",
    "![test3](./output/test3.jpg)\n",
    "![test4](./output/test4.jpg)\n",
    "![test5](./output/test5.jpg)\n",
    "![test6](./output/test6.jpg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Video Implementation\n",
    "\n",
    "#### 1. Here are the vedeo detection results.\n",
    "\n",
    "Here's a [link to project video](./output/project_video_out.mp4)\n",
    "\n",
    "\n",
    "![video-gif](output/project_video_4303.gif)\n",
    "\n",
    "---\n",
    "\n",
    "Here's a [link to test video](./output/test_video_out.mp4)\n",
    "\n",
    "![video-gif](output/test_video_out.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Advanced detection\n",
    "\n",
    "I recorded the positions of positive detections in each frame of the video.  From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected, and retained only those blobs with some threshold (e.g. at least 3 ioverlapping boxes) of overlapping boxes. \n",
    "\n",
    "\n",
    "#### 3. Lane lines\n",
    "\n",
    "Lane lines using code developed for project 4 - advanced lane lines - following is the overview:\n",
    "\n",
    "1. calibrate, undistort frame\n",
    "\n",
    "![undistort](./images/undistort.png)\n",
    "\n",
    "2. apply color, gradient thresholds\n",
    "\n",
    "![threshold](./images/threshold.png)\n",
    "\n",
    "3. perspective transform\n",
    "\n",
    "\n",
    "![perspective](./images/perspectivetr.png)\n",
    "\n",
    "4. detectect lane starting points from histogram\n",
    "\n",
    "![hist](./images/hist.png)\n",
    "\n",
    "\n",
    "\n",
    "### Here are six frames and their corresponding heatmaps:\n",
    "\n",
    "![test1](./images/test1-heat.png)\n",
    "\n",
    "---\n",
    "\n",
    "![test2](./images/test2-heat.png)\n",
    "\n",
    "---\n",
    "\n",
    "![test3](./images/test3-heat.png)\n",
    "\n",
    "---\n",
    "\n",
    "![test4](./images/test4.png)\n",
    "\n",
    "---\n",
    "\n",
    "![test5](./images/test5-heat.png)\n",
    "\n",
    "---\n",
    "\n",
    "![test6](./images/test6-heat.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### This conventional computer vision approach involves many trial and error manually tuned buttons which makes the process a lot tedious, at the end of which there are still many false positives. The newer deep learning approaches such as YOLO, SSD, etc seem a lot more promissing with realtime detection and tracking. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (udacity-carnd)",
   "language": "python",
   "name": "udacity-carnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
